{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train w/ PAR only...\n",
      "Remaining training samples:  1327\n",
      "Remaining testing data:  584\n",
      "1327 \n",
      "584 \n",
      "The result:                                                 model   ACC  BACC    F1  Sens  \\\n",
      "0  data2vec-audio-large-960h_ori_INV_False_min sp...  0.75  0.75  0.75  0.75   \n",
      "\n",
      "   Spec   UAR  \n",
      "0  0.75  0.75  \n"
     ]
    }
   ],
   "source": [
    "# predict AD using (masked) embeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import argparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from utils import ID2Label\n",
    "import os\n",
    "import pickle\n",
    "def trainSVM(x_train, y_train, x_test, y_test, df_test, title, outdir=\"./saves/results/SVM\"):\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(x_train)\n",
    "    x_train_std = sc.transform(x_train)\n",
    "    x_test_std = sc.transform(x_test)\n",
    "    \n",
    "    svm = SVC() #class_weight='balanced')\n",
    "    svm.fit(x_train_std, y_train[\"dementia_labels\"].values)\n",
    "    \n",
    "    pred = svm.predict(x_test_std)\n",
    "    true = y_test[\"dementia_labels\"].values\n",
    "    \"\"\"\n",
    "    # utt-wise results\n",
    "    cm = confusion_matrix(true, pred)\n",
    "    \n",
    "    # save results\n",
    "    df = pd.read_csv(\"./saves/results/SVM/results.csv\")                           # read in previous results\n",
    "    new_row = {'model': title + \" utt-wise\",\n",
    "               'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "               'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                  # result to save\n",
    "    df2 = pd.DataFrame([new_row])\n",
    "    df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "    df3.to_csv(\"./saves/results/SVM/results.csv\", index=False)\n",
    "    \"\"\"\n",
    "    sorted_dict = {}                                                              # sort based on spk id\n",
    "    for idx, i in enumerate(df_test.index.tolist()): \n",
    "        id_part = df_test['path'][i].split('_')                                   # split file name   \n",
    "        if id_part[1] == 'PAR':                                                   # predict only on participant\n",
    "            if id_part[0] not in sorted_dict.keys():                              # new spk\n",
    "                sorted_dict[id_part[0]] = [pred[idx]]                             # add values to this spk\n",
    "            else:\n",
    "                sorted_dict[id_part[0]].append(pred[idx])                         # append to existing list\n",
    "\n",
    "    true = []                                                                     # ground truth\n",
    "    pred = []                                                                     # prediction\n",
    "    for spkid in sorted_dict.keys():                                              # for each spk\n",
    "        true_label = ID2Label(spkid + '_PAR')                                     # get his/her label\n",
    "        true.append(true_label)                                                   # add to list\n",
    "\n",
    "        vote = sum(sorted_dict[spkid]) / len(sorted_dict[spkid])                  # average result of predictions\n",
    "        if vote > 0.5:                                                            # over half of the pred is AD\n",
    "            pred.append(1)                                                        # view as AD\n",
    "        else:\n",
    "            pred.append(0)                                                        # view as HC\n",
    "    \n",
    "    cm = confusion_matrix(true, pred)\n",
    "    \n",
    "    # save results\n",
    "    df = pd.read_csv(f\"{outdir}/results.csv\")                           # read in previous results\n",
    "    new_row = {'model': title + \" spkid-wise\",\n",
    "               'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "               'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                  # result to save\n",
    "    df2 = pd.DataFrame([new_row])\n",
    "    df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "    print(\"The result: \", df2)\n",
    "    df3.to_csv(f\"{outdir}/results.csv\", index=False)\n",
    "\n",
    "def df2xy(df_data, feat_col=\"hidden_states\"):\n",
    "    re = df_data.hidden_states.copy()\n",
    "    for idx, i in enumerate(df_data.index.tolist()):\n",
    "        re[i] = pooling_func(re[i], axis=0)# 平均成(1, hidden_size)\n",
    "        #  = data[0]                                                      # 轉成(hidden_size)\n",
    "        #print(re[i].shape)0\n",
    "        print(\"\\r\"+ str(idx+1), end=\"\")\n",
    "    print(\" \")\n",
    "    x_train = pd.DataFrame(re, columns=[feat_col]).hidden_states.tolist() # masked_hidden_states to list\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "    return x_train,y_train\n",
    "def df2xy_masked(df_data, feat_col=\"masked_hidden_states\"):\n",
    "    # re = df_train.hidden_states * df_train.lm_mask\n",
    "    re = df_data.hidden_states.copy()  * df_data.lm_mask.copy()\n",
    "    for idx, i in enumerate(df_data.index.tolist()):\n",
    "        re[i] = pooling_func(re[i], axis=0)# 平均成(1, hidden_size)\n",
    "        #  = data[0]                                                      # 轉成(hidden_size)\n",
    "        #print(re[i].shape)0\n",
    "        print(\"\\r\"+ str(idx+1), end=\"\")\n",
    "    print(\" \")\n",
    "    x_train = pd.DataFrame(re, columns=[feat_col]).hidden_states.tolist() # masked_hidden_states to list\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "    return x_train,y_train\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-model', '--model_name', type=str, default=\"data2vec-audio-large-960h\", help=\"name of the desired model, ex: FSM_indiv_AMLoss_5_lmFSM\")\n",
    "parser.add_argument('-INV', '--INV', action='store_true', default=False, help=\"True: train w/ INV\")\n",
    "parser.add_argument('-sq', '--squeeze', type=str, default=\"min\", help=\"way to squeeze hidden_states, 'mean', 'min', and 'max'\")\n",
    "parser.add_argument('-dataIn', '--dataIn_dir', type=str, default=\"./saves/results/\", help=\"\")\n",
    "parser.add_argument('-rsltOut', '--rsltOut_dir', type=str, default=\"./saves/results/SVM\", help=\"\")\n",
    "args = parser.parse_args(args=[])\n",
    "sqz = args.squeeze\n",
    "\n",
    "\n",
    "Str2Func={\n",
    "    \"mean\":np.mean,\n",
    "    \"min\":np.min,\n",
    "    \"max\":np.max,\n",
    "    \"median\":np.median,\n",
    "}\n",
    "pooling_func=Str2Func[sqz]\n",
    "\n",
    "suffix='.pkl'\n",
    "if not os.path.exists(args.rsltOut_dir):\n",
    "    os.makedirs(args.rsltOut_dir)\n",
    "# load in train / test data for certain model\n",
    "# df_train = pd.read_csv(f\"{args.dataIn_dir}\" + args.model_name + \"_train.csv\")\n",
    "# df_train = pd.read_csv(f\"{args.dataIn_dir}\" + args.model_name + \".csv\") \n",
    "# df_test = pd.read_csv(f\"{args.dataIn_dir}\" + args.model_name + \".csv\")\n",
    "\n",
    "with open(f\"{args.dataIn_dir}\" + args.model_name + '_train' + suffix, \"rb\") as f:# !!!!!!!!!!!Debug而已 要改回_train.pkl\n",
    "    df_train = pickle.load(f)\n",
    "with open(f\"{args.dataIn_dir}\" + args.model_name + '' + suffix, \"rb\") as f:\n",
    "    df_test = pickle.load(f)\n",
    "\n",
    "if not args.INV:\n",
    "    print(\"Train w/ PAR only...\")\n",
    "    df_train = df_train[df_train.path.str.contains(\"PAR\")]                        # train w/ PAR only\n",
    "    print(\"Remaining training samples: \", len(df_train))                          # show # of utt left\n",
    "    df_test = df_test[df_test.path.str.contains(\"PAR\")]                           # train w/ PAR only\n",
    "    print(\"Remaining testing data: \", len(df_test))                               # show # of utt left\n",
    "\n",
    "\n",
    "# 準備資料\n",
    "if 'lm_mask' in df_train.columns:                                             # model w/ mask\n",
    "    # re = df_train.hidden_states * df_train.lm_mask                            # 得到masked hidden_states  \n",
    "    # x_train,y_train=df2xy_masked(df_train,feat_col=\"masked_hidden_states\")\n",
    "    # re = df_train.hidden_states * df_train.lm_mask\n",
    "    df_data=df_train\n",
    "    feat_col=\"masked_hidden_states\"\n",
    "    re = df_data.hidden_states.copy()  * df_data.lm_mask.copy()\n",
    "    for idx, i in enumerate(df_data.index.tolist()):\n",
    "        re[i] = pooling_func(re[i], axis=0)# 平均成(1, hidden_size)\n",
    "        #  = data[0]                                                      # 轉成(hidden_size)\n",
    "        #print(re[i].shape)0\n",
    "        print(\"\\r\"+ str(idx+1), end=\"\")\n",
    "    print(\" \")\n",
    "    x_train = pd.DataFrame(re, columns=[feat_col]).hidden_states.tolist() # masked_hidden_states to list\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "\n",
    "\n",
    "    # re = df_test.hidden_states * df_test.lm_mask                              # 得到masked hidden_states  \n",
    "    x_test,y_test=df2xy_masked(df_test, feat_col=\"masked_hidden_states\")\n",
    "    # for idx, i in enumerate(df_test.index.tolist()):\n",
    "    #     if sqz == \"mean\":\n",
    "    #         re[i] = np.mean(re[i], axis=1)                                    # 平均成(1, hidden_size)\n",
    "    #     elif sqz == \"min\":\n",
    "    #         re[i] = np.min(re[i], axis=1)                                     # 取min，成(1, hidden_size)\n",
    "    #     elif sqz == \"max\":\n",
    "    #         re[i] = np.max(re[i], axis=1)                                     # 取max，成(1, hidden_size)\n",
    "    #     elif sqz == \"median\":\n",
    "    #         re[i] = np.median(re[i], axis=1)                                  # 取median，成(1, hidden_size)\n",
    "            \n",
    "    #     re[i] = re[i][0]                                                      # 轉成(hidden_size)\n",
    "    #     #print(re[i].shape)\n",
    "    #     print(\"\\r\"+ str(idx+1), end=\"\")\n",
    "    # x_test = pd.DataFrame(re, columns=[\"masked_hidden_states\"]).masked_hidden_states.tolist() # masked_hidden_states to list\n",
    "    # y_test = pd.DataFrame(df_test[\"dementia_labels\"])\n",
    "    # trainSVM(x_train, y_train, x_test, y_test, df_test, args.model_name + \"_masked_INV_\" + str(args.INV) + \"_\" + sqz, outdir=args.rsltOut_dir)\n",
    "else:                                                                         # train w/ un-masked emb.\n",
    "    # x_train,y_train=df2xy(df_train, feat_col=\"hidden_states\")\n",
    "    df_data=df_train\n",
    "    feat_col=\"hidden_states\"\n",
    "    re = df_data.hidden_states.copy()\n",
    "    for idx, i in enumerate(df_data.index.tolist()):\n",
    "        re[i] = pooling_func(re[i], axis=0)# 平均成(1, hidden_size)\n",
    "        #  = data[0]                                                      # 轉成(hidden_size)\n",
    "        #print(re[i].shape)0\n",
    "        print(\"\\r\"+ str(idx+1), end=\"\")\n",
    "    print(\" \")\n",
    "    x_train = pd.DataFrame(re, columns=[feat_col]).hidden_states.tolist() # masked_hidden_states to list\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "    x_test,y_test=df2xy(df_test, feat_col=\"hidden_states\")\n",
    "\n",
    "\n",
    "\n",
    "# Train SVM start here\n",
    "title=args.model_name + \"_ori_INV_\" + str(args.INV) + \"_\" + sqz\n",
    "outdir=args.rsltOut_dir\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "svm = SVC() #class_weight='balanced')\n",
    "svm.fit(x_train_std, y_train[\"dementia_labels\"].values)\n",
    "\n",
    "pred = svm.predict(x_test_std)\n",
    "true = y_test[\"dementia_labels\"].values\n",
    "\"\"\"\n",
    "# utt-wise results\n",
    "cm = confusion_matrix(true, pred)\n",
    "\n",
    "# save results\n",
    "df = pd.read_csv(\"./saves/results/SVM/results.csv\")                           # read in previous results\n",
    "new_row = {'model': title + \" utt-wise\",\n",
    "            'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "            'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                # result to save\n",
    "df2 = pd.DataFrame([new_row])\n",
    "df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "df3.to_csv(\"./saves/results/SVM/results.csv\", index=False)\n",
    "\"\"\"\n",
    "sorted_dict = {}                                                              # sort based on spk id\n",
    "for idx, i in enumerate(df_test.index.tolist()): \n",
    "    id_part = df_test['path'][i].split('_')                                   # split file name   \n",
    "    if id_part[1] == 'PAR':                                                   # predict only on participant\n",
    "        if id_part[0] not in sorted_dict.keys():                              # new spk\n",
    "            sorted_dict[id_part[0]] = [pred[idx]]                             # add values to this spk\n",
    "        else:\n",
    "            sorted_dict[id_part[0]].append(pred[idx])                         # append to existing list\n",
    "\n",
    "true = []                                                                     # ground truth\n",
    "pred = []                                                                     # prediction\n",
    "for spkid in sorted_dict.keys():                                              # for each spk\n",
    "    true_label = ID2Label(spkid + '_PAR')                                     # get his/her label\n",
    "    true.append(true_label)                                                   # add to list\n",
    "\n",
    "    vote = sum(sorted_dict[spkid]) / len(sorted_dict[spkid])                  # average result of predictions\n",
    "    if vote > 0.5:                                                            # over half of the pred is AD\n",
    "        pred.append(1)                                                        # view as AD\n",
    "    else:\n",
    "        pred.append(0)                                                        # view as HC\n",
    "\n",
    "cm = confusion_matrix(true, pred)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "# save results\n",
    "prev_savedFile=f\"{outdir}/results.csv\"\n",
    "if not os.path.exists(prev_savedFile):\n",
    "    df=pd.DataFrame()\n",
    "else:\n",
    "    df=pd.read_csv(prev_savedFile)  # read in previous results\n",
    "new_row = {'model': title + \" spkid-wise\",\n",
    "            'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "            'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                # result to save\n",
    "df2 = pd.DataFrame([new_row])\n",
    "print(\"The result: \", df2)\n",
    "df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "df3.to_csv(f\"{outdir}/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          hidden_states\n",
      "0     [-0.8868486, -0.61396736, -0.81702024, -0.8143...\n",
      "1     [-0.80009747, -0.50253224, -0.6598102, -1.1434...\n",
      "2     [-0.7763984, -0.5658758, -0.6632675, -0.471799...\n",
      "3     [-0.7177696, -1.1432803, -0.93652266, -1.35663...\n",
      "4     [-0.75240654, -0.7389151, -0.65402615, -1.0788...\n",
      "...                                                 ...\n",
      "1863  [-0.6320326, -0.42740813, -0.7096527, -0.79593...\n",
      "1864  [-0.6037457, -0.3633005, -0.514737, -0.4930829...\n",
      "1865  [-0.9807792, -1.0697064, -0.8333728, -1.287738...\n",
      "1866  [-0.73122627, -0.34061745, -0.57485473, -1.053...\n",
      "1867  [-0.9009252, -0.8197352, -0.97118765, -1.03419...\n",
      "\n",
      "[1868 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# df_train.hidden_states\n",
    "# print(df_data.hidden_states)\n",
    "# print(df_data.hidden_states.loc[0])\n",
    "print(pd.DataFrame(re, columns=[feat_col]))\n",
    "# print(len(re[i]))\n",
    "# print(x_train[0])\n",
    "# print(len(x_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flower-speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
