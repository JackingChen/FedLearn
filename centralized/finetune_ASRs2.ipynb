{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 更新2023/04/10\n",
    "# 1. csv2dataset函數裡面使用csv_path和root_path\n",
    "# 2. 在讀音檔的時候增加一個選項：scipy.io，讀起來會快很多但是不知道會不會影響到原來的效果\n",
    "\n",
    "# 大約10138MiB\n",
    "# =============================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import librosa\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "from transformers import Data2VecAudioConfig, HubertConfig, SEWDConfig, UniSpeechSatConfig\n",
    "from transformers import Data2VecAudioForCTC, HubertForCTC, SEWDForCTC, UniSpeechSatForCTC\n",
    "from jiwer import wer\n",
    "import scipy.io\n",
    "from utils import csv2dataset, WriteResult\n",
    "\n",
    "# set up trainer\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"array\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = new_model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = new_processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = new_processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('-model', '--model_path', type=str, default=\"./saves/wav2vec2-base-960h_GRL_0.5\", help=\"Where the model is saved\")\n",
    "parser.add_argument('-opt', '--optimizer', type=str, default=\"adamw_hf\", help=\"The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor\")\n",
    "parser.add_argument('-MGN', '--max_grad_norm', type=float, default=1.0, help=\"Maximum gradient norm (for gradient clipping)\")\n",
    "parser.add_argument('-model_type', '--model_type', type=str, default=\"data2vec\", help=\"Type of the model\")\n",
    "parser.add_argument('-sr', '--sampl_rate', type=float, default=16000, help=\"librosa read smping rate\")\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=1e-5, help=\"Learning rate\")\n",
    "parser.add_argument('-RD', '--root_dir', default='/mnt/Internal/FedASR/Data/ADReSS-IS2020-data', help=\"Learning rate\")\n",
    "parser.add_argument('--AudioLoadFunc', default='librosa', help=\"用scipy function好像可以比較快\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "\n",
    "#model_out_dir = args.model_path # where to save model\n",
    "model_type = args.model_type                # what type of the model\n",
    "lr = args.learning_rate                     # learning rate\n",
    "optim = args.optimizer                      # opt\n",
    "max_grad_norm = args.max_grad_norm          # max_grad_norm\n",
    "\n",
    "\n",
    "# load in train-dev-test\n",
    "train_data = csv2dataset(audio_path = '{}/clips/'.format(args.root_dir),\n",
    "                         csv_path = \"{}/mid_csv/train.csv\".format(args.root_dir)) #!!! librosa在load的時候非常慢，大約7分47秒讀完1869個file\n",
    "dev_data = csv2dataset(audio_path = '{}/clips/'.format(args.root_dir),\n",
    "                       csv_path = \"{}/mid_csv/dev.csv\".format(args.root_dir))\n",
    "test_data = csv2dataset(audio_path = '{}/clips/'.format(args.root_dir),\n",
    "                        csv_path = \"{}/mid_csv/test.csv\".format(args.root_dir))\n",
    "\n",
    "if model_type == \"wav2vec\":\n",
    "    name = \"facebook/wav2vec2-base-960h\" # + model_dir.split(\"/\")[-3]\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(name)\n",
    "    print(\"Current model: \", name)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"data2vec\":\n",
    "    name = \"facebook/data2vec-audio-large-960h\" # + model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    mask_time_prob = 0                                                                     # change config\n",
    "    config = Data2VecAudioConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = Data2VecAudioForCTC.from_pretrained(name, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"hubert\":\n",
    "    name = \"facebook/hubert-xlarge-ls960-ft\" # + model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    mask_time_prob = 0                                                                     # change config\n",
    "    config = HubertConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = HubertForCTC.from_pretrained(name, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"sewd\":\n",
    "    name = \"asapp/sew-d-mid-400k-ft-ls100h\" #+ model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    mask_time_prob = 0                                                                     # change config\n",
    "    config = SEWDConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = SEWDForCTC.from_pretrained(name, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"unispeech\":\n",
    "    name = \"microsoft/unispeech-sat-base-100h-libri-ft\" # + model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    mask_time_prob = 0                                                                     # change config\n",
    "    config = UniSpeechSatConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = UniSpeechSatForCTC.from_pretrained(name, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "else:\n",
    "    print(\"WRONG TYPE!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "\n",
    "# use processor to get labels\n",
    "# 在這段程式碼中，map() 是一個運用在 test_data 上的函式，它的目的是將 test_data 中的每個元素都應用到 map_to_result 函式上，並生成一個新的結果序列。\n",
    "# datasets object 通常使用.map()函數更改裡面預設的變數\n",
    "train_data = train_data.map(prepare_dataset, num_proc=4)\n",
    "dev_data = dev_data.map(prepare_dataset, num_proc=4)\n",
    "test_data = test_data.map(prepare_dataset, num_proc=4)\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./saves/\" + name.split(\"/\")[-1] + \"_finetuned\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True, \n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.005,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    optim=optim,\n",
    "    max_grad_norm=max_grad_norm,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=dev_data,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "trainer.train()\n",
    "Save_path=\"./saves/\" + name.split(\"/\")[-1] + \"_finetuned/final\"\n",
    "trainer.save_model(Save_path)\n",
    "\n",
    "# load in trained model\n",
    "if model_type == \"wav2vec\":\n",
    "    new_model = Wav2Vec2ForCTC.from_pretrained(Save_path)\n",
    "    new_processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"data2vec\":\n",
    "    new_model = Data2VecAudioForCTC.from_pretrained(Save_path)\n",
    "    new_processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"hubert\":\n",
    "    new_model = HubertForCTC.from_pretrained(Save_path)\n",
    "    new_processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"sewd\":\n",
    "    new_model = SEWDForCTC.from_pretrained(Save_path)\n",
    "    new_processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"unispeech\":\n",
    "    new_model = UniSpeechSatForCTC.from_pretrained(Save_path)\n",
    "    new_processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "else:\n",
    "    print(\"WRONG TYPE!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "result = test_data.map(map_to_result)\n",
    "print(\"WER of \", name, \" : \", wer(result[\"text\"], result[\"pred_str\"]))\n",
    "WriteResult(result,Save_path)\n",
    "print(\"DONE!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
