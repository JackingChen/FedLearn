{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 15:38:55.116250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/ipykernel_launcher.py:89: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model:  facebook/data2vec-audio-large-960h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-large-960h were not used when initializing Data2VecAudioForCTC: ['data2vec_audio.masked_spec_embed']\n",
      "- This IS expected if you are initializing Data2VecAudioForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecAudioForCTC were not initialized from the model checkpoint at facebook/data2vec-audio-large-960h and are newly initialized: ['dementia_head.weight', 'dementia_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  tensor(0.5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/centralized/dataset/train/cache-684be29c00c58e99_*_of_00010.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/centralized/dataset/test/cache-bbb0167bf98fd901_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n",
      "Load data from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 4 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  \"`as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your \"\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 04:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'ccc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_461408/3529927824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mccc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\"./saves/data2vec-audio-large-960h_GRL/checkpoint-56000/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_log_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ccc' is not defined"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Data2VecAudioModel\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.utils import logging\n",
    "from transformers.models.data2vec.configuration_data2vec_audio import Data2VecAudioConfig\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Data2VecAudioConfig\n",
    "from datasets import load_metric\n",
    "import argparse\n",
    "from utils import csv2dataset\n",
    "\n",
    "from transformers import Trainer\n",
    "from Models import (DataCollatorCTCWithPadding, \n",
    "                    Data2VecAudioForCTC,)\n",
    "import time\n",
    "import os, json\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class DementiaGRLTrainer(Trainer):  \n",
    "    def update_log_file(self, log_file=None):\n",
    "        self.log_file=log_file\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            \"\"\"\n",
    "            How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "            Subclass and override for custom behavior.\n",
    "            \"\"\"\n",
    "            #dementia_labels = inputs.pop(\"dementia_labels\") # pop Âá∫‰æÜÂ∞±ÊúÉ‰∏çË¶ã?\n",
    "            \n",
    "            if self.label_smoother is not None and \"labels\" in inputs:\n",
    "                labels = inputs.pop(\"labels\")\n",
    "            else:\n",
    "                labels = None\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            # Save past state if it exists\n",
    "            # TODO: this needs to be fixed and made cleaner later.\n",
    "            if self.args.past_index >= 0:\n",
    "                self._past = outputs[self.args.past_index]\n",
    "\n",
    "            if labels is not None:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "            else:\n",
    "                # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "                loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.state.log_history.append(output)\n",
    "        # Ë®≠ÂÆölog file‰ΩçÁΩÆËàáÂêçÁ®±\n",
    "\n",
    "        LOG_DIR = './saves/log/'\n",
    "        if not os.path.exists(LOG_DIR):\n",
    "            os.makedirs(LOG_DIR)\n",
    "        # write to txt file\n",
    "        file_object = open(LOG_DIR + self.log_file, 'a')\n",
    "        # Append at the end of file\n",
    "        file_object.write(json.dumps(output) + '\\n')\n",
    "        # Close the file\n",
    "        file_object.close()\n",
    "\n",
    "        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"array\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "        \n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    label_ids_asr , label_ids_AD=pred.label_ids\n",
    "\n",
    "    label_ids_asr[label_ids_asr == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(label_ids_asr, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-lam', '--LAMBDA', type=float, default=0.5, help=\"Lambda for GRL\")\n",
    "parser.add_argument('-st', '--STAGE', type=int, default=1, help=\"Current training stage\")\n",
    "parser.add_argument('-GRL', '--GRL', action='store_true', default=False, help=\"True: GRL\")\n",
    "parser.add_argument('-model_in', '--model_in_path', type=str, default=\"/mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h/final/\", help=\"Where the model is saved\")\n",
    "parser.add_argument('-model_out', '--model_out_path', type=str, default=\"./saves/data2vec2-base-960h_linear_GRL\", help=\"Where to save the model\")\n",
    "parser.add_argument('-log', '--log_path', type=str, default=\"data2vec2-base-960h_linear_GRL.txt\", help=\"name for the txt file\")\n",
    "args = parser.parse_args(args=[])\n",
    "LAMBDA = args.LAMBDA                    # lambda for GRL\n",
    "REVERSE = args.GRL                      # not used in this version\n",
    "STAGE = args.STAGE                      # stage 1: train AD classifier; stage 2: train toggling network\n",
    "model_in_dir = args.model_in_path       # path to load the initial model\n",
    "model_out_dir = args.model_out_path     # path to store the resulted model\n",
    "log_file = args.log_path                # path to save log file\n",
    "\n",
    "\n",
    "\n",
    "# threshold for maskes, not used here\n",
    "AD_THRES = 0.5\n",
    "LM_THRES = 0.5\n",
    "\n",
    "# load model from huggingface hub, here data2vec model\n",
    "name = \"facebook/\" + model_in_dir.split(\"/\")[-3]\n",
    "print(\"Current model: \", name)\n",
    "\n",
    "mask_time_prob = 0                                         # change config to avoid training stopping\n",
    "config = Data2VecAudioConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "model = Data2VecAudioForCTC.from_pretrained(name, config=config,LAMBDA=LAMBDA)\n",
    "processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# load train / test data\n",
    "train_data = csv2dataset(csv_path = \"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/mid_csv/train.csv\")\n",
    "#dev_data = csv2dataset(path = \"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/mid_csv/dev.csv\")\n",
    "test_data = csv2dataset(csv_path = \"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/mid_csv/test.csv\")\n",
    "\n",
    "# map to desired form\n",
    "train_data = train_data.map(prepare_dataset, num_proc=10)\n",
    "#dev_data = dev_data.map(prepare_dataset, num_proc=10)\n",
    "test_data = test_data.map(prepare_dataset, num_proc=10)\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_out_dir,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=30,                 # finetune & GRL\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True, \n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100000000,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.005,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    # log_level='debug',\n",
    "    logging_strategy=\"steps\",\n",
    "    optim=\"adafactor\", #adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
    "    #adafactor=False,            # default:false. Whether or not to use transformers.Adafactor optimizer instead of transformers.AdamW\n",
    "    #fp16_full_eval=True,      # to save memory\n",
    "    max_grad_norm=0.5\n",
    ")\n",
    "\n",
    "trainer = DementiaGRLTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data, \n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "trainer.update_log_file(log_file=log_file)\n",
    "# trainer.evaluate(eval_dataset=test_data.select(range(2)))\n",
    "trainer.evaluate(eval_dataset=test_data)\n",
    "###############################################\n",
    "# eval_dataset=test_data\n",
    "# trainer._memory_tracker.start()\n",
    "\n",
    "# eval_dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "# start_time = time.time()\n",
    "\n",
    "# eval_loop = trainer.prediction_loop if trainer.args.use_legacy_prediction_loop else trainer.evaluation_loop\n",
    "# output = eval_loop(\n",
    "#     eval_dataloader,\n",
    "#     description=\"Evaluation\",\n",
    "#     # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "#     # self.args.prediction_loss_only\n",
    "#     prediction_loss_only=True if trainer.compute_metrics is None else None,\n",
    "#     ignore_keys=None,\n",
    "#     metric_key_prefix=None,\n",
    "# )\n",
    "###########################################################\n",
    "# eval_dataset=test_data\n",
    "# trainer._memory_tracker.start()\n",
    "\n",
    "# eval_dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "# start_time = time.time()\n",
    "# eval_dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "# from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n",
    "# from transformers.trainer_utils import (\n",
    "#     PREFIX_CHECKPOINT_DIR,\n",
    "#     BestRun,\n",
    "#     EvalLoopOutput,\n",
    "#     EvalPrediction,\n",
    "#     FSDPOption,\n",
    "#     HPSearchBackend,\n",
    "#     HubStrategy,\n",
    "#     IntervalStrategy,\n",
    "#     PredictionOutput,\n",
    "#     RemoveColumnsCollator,\n",
    "#     ShardedDDPOption,\n",
    "#     TrainerMemoryTracker,\n",
    "#     TrainOutput,\n",
    "#     default_compute_objective,\n",
    "#     default_hp_space,\n",
    "#     denumpify_detensorize,\n",
    "#     enable_full_determinism,\n",
    "#     find_executable_batch_size,\n",
    "#     get_last_checkpoint,\n",
    "#     has_length,\n",
    "#     number_of_arguments,\n",
    "#     seed_worker,\n",
    "#     set_seed,\n",
    "#     speed_metrics,\n",
    "# )\n",
    "# from transformers.utils import (\n",
    "#     CONFIG_NAME,\n",
    "#     SAFE_WEIGHTS_INDEX_NAME,\n",
    "#     SAFE_WEIGHTS_NAME,\n",
    "#     WEIGHTS_INDEX_NAME,\n",
    "#     WEIGHTS_NAME,\n",
    "#     can_return_loss,\n",
    "#     find_labels,\n",
    "#     get_full_repo_name,\n",
    "#     is_accelerate_available,\n",
    "#     is_apex_available,\n",
    "#     is_datasets_available,\n",
    "#     is_in_notebook,\n",
    "#     is_ipex_available,\n",
    "#     is_safetensors_available,\n",
    "#     is_sagemaker_dp_enabled,\n",
    "#     is_sagemaker_mp_enabled,\n",
    "#     is_torch_compile_available,\n",
    "#     is_torch_neuroncore_available,\n",
    "#     is_torch_tpu_available,\n",
    "#     logging,\n",
    "#     strtobool,\n",
    "# )\n",
    "# from transformers.trainer_pt_utils import (\n",
    "#     DistributedLengthGroupedSampler,\n",
    "#     DistributedSamplerWithLoop,\n",
    "#     DistributedTensorGatherer,\n",
    "#     IterableDatasetShard,\n",
    "#     LabelSmoother,\n",
    "#     LengthGroupedSampler,\n",
    "#     SequentialDistributedSampler,\n",
    "#     ShardSampler,\n",
    "#     distributed_broadcast_scalars,\n",
    "#     distributed_concat,\n",
    "#     find_batch_size,\n",
    "#     get_model_param_count,\n",
    "#     get_module_class_from_name,\n",
    "#     get_parameter_names,\n",
    "#     nested_concat,\n",
    "#     nested_detach,\n",
    "#     nested_numpify,\n",
    "#     nested_truncate,\n",
    "#     nested_xla_mesh_reduce,\n",
    "#     reissue_pt_warnings,\n",
    "# )\n",
    "# if is_torch_tpu_available(check_device=False):\n",
    "#     import torch_xla.core.xla_model as xm\n",
    "#     import torch_xla.debug.metrics as met\n",
    "#     import torch_xla.distributed.parallel_loader as pl\n",
    "# import torch\n",
    "# # ========== Init\n",
    "# args = trainer.args\n",
    "# dataloader=eval_dataloader\n",
    "# description=\"Evaluation\"\n",
    "# prediction_loss_only=True if trainer.compute_metrics is None else None\n",
    "# ignore_keys=None\n",
    "# metric_key_prefix=None\n",
    "# # ===========================\n",
    "\n",
    "\n",
    "# prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
    "\n",
    "# # if eval is called w/o train init deepspeed here\n",
    "# if args.deepspeed and not trainer.deepspeed:\n",
    "#     # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n",
    "#     # from the checkpoint eventually\n",
    "#     deepspeed_engine, _, _ = deepspeed_init(\n",
    "#         trainer, num_training_steps=0, resume_from_checkpoint=None, inference=True\n",
    "#     )\n",
    "#     trainer.model = deepspeed_engine.module\n",
    "#     trainer.model_wrapped = deepspeed_engine\n",
    "#     trainer.deepspeed = deepspeed_engine\n",
    "\n",
    "# model = trainer._wrap_model(trainer.model, training=False, dataloader=dataloader)\n",
    "\n",
    "# # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n",
    "# # while ``train`` is running, cast it to the right dtype first and then put on device\n",
    "# if not trainer.is_in_train:\n",
    "#     if args.fp16_full_eval:\n",
    "#         model = model.to(dtype=torch.float16, device=args.device)\n",
    "#     elif args.bf16_full_eval:\n",
    "#         model = model.to(dtype=torch.bfloat16, device=args.device)\n",
    "\n",
    "# batch_size = trainer.args.eval_batch_size\n",
    "\n",
    "# logger.info(f\"***** Running {description} *****\")\n",
    "# if has_length(dataloader):\n",
    "#     logger.info(f\"  Num examples = {trainer.num_examples(dataloader)}\")\n",
    "# else:\n",
    "#     logger.info(\"  Num examples: Unknown\")\n",
    "# logger.info(f\"  Batch size = {batch_size}\")\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# trainer.callback_handler.eval_dataloader = dataloader\n",
    "# # Do this before wrapping.\n",
    "# eval_dataset = getattr(dataloader, \"dataset\", None)\n",
    "\n",
    "# if is_torch_tpu_available():\n",
    "#     dataloader = pl.ParallelLoader(dataloader, [args.device]).per_device_loader(args.device)\n",
    "\n",
    "# if args.past_index >= 0:\n",
    "#     trainer._past = None\n",
    "\n",
    "# # Initialize containers\n",
    "# # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
    "# losses_host = None\n",
    "# preds_host = None\n",
    "# labels_host = None\n",
    "# inputs_host = None\n",
    "\n",
    "# # losses/preds/labels on CPU (final containers)\n",
    "# all_losses = None\n",
    "# all_preds = None\n",
    "# all_labels = None\n",
    "# all_inputs = None\n",
    "# # Will be useful when we have an iterable dataset so don't know its length.\n",
    "\n",
    "# observed_num_examples = 0\n",
    "# # Main evaluation loop\n",
    "# for step, inputs in enumerate(dataloader):\n",
    "#     # Update the observed num examples\n",
    "#     observed_batch_size = find_batch_size(inputs)\n",
    "#     if observed_batch_size is not None:\n",
    "#         observed_num_examples += observed_batch_size\n",
    "#         # For batch samplers, batch_size is not known by the dataloader in advance.\n",
    "#         if batch_size is None:\n",
    "#             batch_size = observed_batch_size\n",
    "\n",
    "#     # Prediction step\n",
    "#     loss, logits, labels = trainer.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "#     inputs_decode = trainer._prepare_input(inputs[\"input_ids\"]) if args.include_inputs_for_metrics else None\n",
    "#     # print(loss, logits, labels)\n",
    "#     # print(inputs_decode)\n",
    "#     if is_torch_tpu_available():\n",
    "#         xm.mark_step()\n",
    "\n",
    "#     # Update containers on host\n",
    "#     if loss is not None:\n",
    "#         losses = trainer._nested_gather(loss.repeat(batch_size))\n",
    "#         losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n",
    "#     if labels is not None:\n",
    "#         labels = trainer._pad_across_processes(labels)\n",
    "#         labels = trainer._nested_gather(labels)\n",
    "#         labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
    "#     if inputs_decode is not None:\n",
    "#         inputs_decode = trainer._pad_across_processes(inputs_decode)\n",
    "#         inputs_decode = trainer._nested_gather(inputs_decode)\n",
    "#         inputs_host = (\n",
    "#             inputs_decode\n",
    "#             if inputs_host is None\n",
    "#             else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n",
    "#         )\n",
    "#     if logits is not None:\n",
    "#         logits = trainer._pad_across_processes(logits)\n",
    "#         logits = trainer._nested_gather(logits)\n",
    "#         if trainer.preprocess_logits_for_metrics is not None:\n",
    "#             logits = trainer.preprocess_logits_for_metrics(logits, labels)\n",
    "#         preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n",
    "#     trainer.control = trainer.callback_handler.on_prediction_step(args, trainer.state, trainer.control)\n",
    "\n",
    "#     # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
    "#     if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n",
    "#         if losses_host is not None:\n",
    "#             losses = nested_numpify(losses_host)\n",
    "#             all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "#         if preds_host is not None:\n",
    "#             logits = nested_numpify(preds_host)\n",
    "#             all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "#         if inputs_host is not None:\n",
    "#             inputs_decode = nested_numpify(inputs_host)\n",
    "#             all_inputs = (\n",
    "#                 inputs_decode\n",
    "#                 if all_inputs is None\n",
    "#                 else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n",
    "#             )\n",
    "#         if labels_host is not None:\n",
    "#             labels = nested_numpify(labels_host)\n",
    "#             all_labels = (\n",
    "#                 labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "#             )\n",
    "\n",
    "#         # Set back to None to begin a new accumulation\n",
    "#         losses_host, preds_host, inputs_host, labels_host = None, None, None, None\n",
    "\n",
    "# if args.past_index and hasattr(trainer, \"_past\"):\n",
    "#     # Clean the state at the end of the evaluation loop\n",
    "#     delattr(trainer, \"_past\")\n",
    "\n",
    "# # Gather all remaining tensors and put them back on the CPU\n",
    "# if losses_host is not None:\n",
    "#     losses = nested_numpify(losses_host)\n",
    "#     all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "# if preds_host is not None:\n",
    "#     logits = nested_numpify(preds_host)\n",
    "#     all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "# if inputs_host is not None:\n",
    "#     inputs_decode = nested_numpify(inputs_host)\n",
    "#     all_inputs = (\n",
    "#         inputs_decode if all_inputs is None else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n",
    "#     )\n",
    "# if labels_host is not None:\n",
    "#     labels = nested_numpify(labels_host)\n",
    "#     all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "\n",
    "# # Number of samples\n",
    "# if has_length(eval_dataset):\n",
    "#     num_samples = len(eval_dataset)\n",
    "# # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n",
    "# # methods. Therefore we need to make sure it also has the attribute.\n",
    "# elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n",
    "#     num_samples = eval_dataset.num_examples\n",
    "# else:\n",
    "#     if has_length(dataloader):\n",
    "#         num_samples = trainer.num_examples(dataloader)\n",
    "#     else:  # both len(dataloader.dataset) and len(dataloader) fail\n",
    "#         num_samples = observed_num_examples\n",
    "# if num_samples == 0 and observed_num_examples > 0:\n",
    "#     num_samples = observed_num_examples\n",
    "\n",
    "# # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
    "# # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
    "# if all_losses is not None:\n",
    "#     all_losses = all_losses[:num_samples]\n",
    "# if all_preds is not None:\n",
    "#     all_preds = nested_truncate(all_preds, num_samples)\n",
    "# if all_labels is not None:\n",
    "#     all_labels = nested_truncate(all_labels, num_samples)\n",
    "# if all_inputs is not None:\n",
    "#     all_inputs = nested_truncate(all_inputs, num_samples)\n",
    "\n",
    "# # Metrics!\n",
    "# if trainer.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
    "#     if args.include_inputs_for_metrics:\n",
    "#         metrics = trainer.compute_metrics(\n",
    "#             EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n",
    "#         )\n",
    "#     else:\n",
    "#         metrics = trainer.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
    "# else:\n",
    "#     metrics = {}\n",
    "\n",
    "\n",
    "aaa=ccc\n",
    "trainer.train() #\"./saves/data2vec-audio-large-960h_GRL/checkpoint-56000/\"\n",
    "trainer.update_log_file(log_file=log_file)\n",
    "trainer.save_model(model_out_dir + \"/final\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compute_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flower-speechbrain",
   "language": "python",
   "name": "flower-speechbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
