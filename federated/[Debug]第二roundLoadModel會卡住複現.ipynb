{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference, ASRLocalUpdate\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, Data2VecAudioForCTC, DataCollatorCTCWithPadding\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "from transformers import Data2VecAudioConfig, Wav2Vec2Processor\n",
    "from multiprocessing import Pool\n",
    "from collections import OrderedDict\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# federated arguments (Notation for the arguments followed from paper)\n",
    "parser.add_argument('--epochs', type=int, default=2,\n",
    "                    help=\"number of rounds of training\")\n",
    "parser.add_argument('--num_users', type=int, default=2,\n",
    "                    help=\"number of users: K\")\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='the fraction of clients: C')\n",
    "parser.add_argument('--local_ep', type=int, default=1,\n",
    "                    help=\"the number of local epochs: E\")\n",
    "\n",
    "parser.add_argument('--model', type=str, default='data2vec', help='model name')\n",
    "\n",
    "\n",
    "# other arguments\n",
    "parser.add_argument('--dataset', type=str, default='adress', help=\"name \\\n",
    "                    of dataset\") #cifar\n",
    "#parser.add_argument('--num_classes', type=int, default=10, help=\"number \\\n",
    "#                    of classes\")\n",
    "parser.add_argument('--gpu', default=1, help=\"To use cuda, set \\\n",
    "                    to a specific GPU ID. Default set to use CPU.\")\n",
    "\n",
    "# additional arguments\n",
    "parser.add_argument('--pretrain_name', type=str, default='facebook/data2vec-audio-large-960h', help=\"str used to load pretrain model\")\n",
    "parser.add_argument('-lam', '--LAMBDA', type=float, default=0.5, help=\"Lambda for GRL\")\n",
    "parser.add_argument('-st', '--STAGE', type=int, default=2, help=\"Current training stage\")\n",
    "parser.add_argument('-GRL', '--GRL', action='store_true', default=False, help=\"True: GRL\")\n",
    "parser.add_argument('-model_in', '--model_in_path', type=str, default=\"/mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h_new1_recall/final/\", help=\"Where the model is saved\")\n",
    "parser.add_argument('-model_out', '--model_out_path', type=str, default=\"./save/data2vec-audio-large-960h_new2_recall_FL\", help=\"Where to save the model\")\n",
    "parser.add_argument('-log', '--log_path', type=str, default=\"data2vec-audio-large-960h_new2_recall_FL.txt\", help=\"name for the txt file\")\n",
    "# 2023/01/08: loss type\n",
    "parser.add_argument('-ad_loss', '--AD_loss', type=str, default=\"recall\", help=\"loss to use for AD classifier\")\n",
    "# 2023/01/18: ckpt\n",
    "parser.add_argument('-ckpt', '--checkpoint', type=str, default=None, help=\"path to checkpoint\")\n",
    "# 2023/02/13: TOGGLE_RATIO\n",
    "parser.add_argument('-toggle_rt', '--TOGGLE_RATIO', type=float, default=0, help=\"To toggle more or less\")\n",
    "# 2023/02/15: GS_TAU, loss weight\n",
    "parser.add_argument('-gs_tau', '--GS_TAU', type=float, default=1, help=\"Tau for gumbel_softmax\")\n",
    "parser.add_argument('-w_loss', '--W_LOSS', type=float, default=None, nargs='+', help=\"weight for HC and AD\")\n",
    "\n",
    "args = parser.parse_args(args=[]) # for jupyter notebook\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # 或者其他你想要使用的 GPU 編號\n",
    "lock = mp.Lock()\n",
    "logger = SummaryWriter('../logs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一步我把logger拿掉了\n",
    "# final_result = pool.starmap_async(\n",
    "#     client_train, [(args, train_dataset, logger,\n",
    "#                     test_dataset, idx, epoch, global_weights)\n",
    "#                 for idx in idxs_users])\n",
    "#傳logger會出現RuntimeError: Queue objects should only be shared between processes through inheritance\n",
    "final_result = pool.starmap_async(\n",
    "    client_train, [(args, train_dataset, None,\n",
    "                    test_dataset, idx, epoch, global_weights)\n",
    "                for idx in idxs_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "def client_train(args, train_dataset, logger,\n",
    "                 test_dataset, idx, epoch, global_weights=None):\n",
    "    if args.model == 'data2vec':\n",
    "        mask_time_prob = 0                                                          # change config to avoid training stopping\n",
    "        config = Data2VecAudioConfig.from_pretrained(args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "        print(\"load from \", args.model_in_path)\n",
    "        model = Data2VecAudioForCTC.from_pretrained(args.model_in_path, config=config, args=args)\n",
    "        print(\"model loaded\")                                                       # load/initialize global model\n",
    "        model.config.ctc_zero_infinity = True                                       # to avoid inf values\n",
    "\n",
    "        global_model = copy.deepcopy(model.arbitrator)                              # only has global toggling network\n",
    "        if global_weights != None:                                                  # if given global_weights\n",
    "            global_model.load_state_dict(global_weights)                            # load it\n",
    "        #else:\n",
    "        #    # copy weights\n",
    "        #    global_weights = copy.deepcopy(global_model.state_dict())                       # save global weight\n",
    "        processor = Wav2Vec2Processor.from_pretrained(args.pretrain_name)\n",
    "        data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "    else:\n",
    "        exit('Error: unrecognized model') \n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    device = 'cuda' if args.gpu else 'cpu'\n",
    "    global_model.to(device)\n",
    "    global_model.train()      \n",
    "\n",
    "    #print(global_model)\n",
    "\n",
    "    ####################\n",
    "    # 'use client_id generate sub-dataset' to be done\n",
    "    ####################\n",
    "    #print(\"call ASRLocalUpdate\")\n",
    "    local_model = ASRLocalUpdate(args=args, dataset=train_dataset, logger=logger,\n",
    "                        data_collator=data_collator, global_test_dataset=test_dataset, \n",
    "                        processor=processor, client_id=idx)\n",
    "                                                                                    # initial dataset of current client\n",
    "    ####################\n",
    "    # 'use client_id load local model' to be done\n",
    "    # 'save model in final round' to be done\n",
    "    ####################\n",
    "    #print(\"perform update_weight\")\n",
    "    w, loss = local_model.update_weights(\n",
    "        global_arbitrator=copy.deepcopy(global_model), global_round=epoch)          # from global model to train\n",
    "    \n",
    "    #send_end.send([w, loss])                                                        # save model weights and average round loss\n",
    "    #return_dict[str(idx)] = [w, loss]\n",
    "    print(\"PID {} Getting \".format(os.getpid()), \"Done\")\n",
    "    return w, loss\n",
    "    \n",
    "                                                                   \n",
    "    return 0\n",
    "        \n",
    "train_loss, test_wer = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "global_weights = None                                                           # initial global_weights\n",
    "\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(2):\n",
    "        m = max(int(args.frac * args.num_users), 1)                                 # num of clients to train, min:1\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)      # select by client_id\n",
    "        pool = multiprocessing.Pool(processes=m)\n",
    "        try:\n",
    "            # final_result = pool.starmap_async(\n",
    "            #     client_train, [(args, train_dataset, logger,\n",
    "            #                     test_dataset, idx, epoch, global_weights)\n",
    "            #                 for idx in idxs_users])\n",
    "            #傳logger會出現RuntimeError: Queue objects should only be shared between processes through inheritance\n",
    "            final_result = pool.starmap_async(\n",
    "                client_train, [(args, train_dataset, None,\n",
    "                                test_dataset, idx, epoch, global_weights)\n",
    "                            for idx in idxs_users])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while running local_model.update_weights(): {str(e)}\")\n",
    "        finally:\n",
    "            final_result.wait()\n",
    "            results = final_result.get()\n",
    "        \n",
    "        local_weights = []\n",
    "        local_losses = []\n",
    "        for idx in range(len(results)):\n",
    "            w, loss = results[idx]\n",
    "            local_weights.append(w)\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        print(\"local weights: \", local_weights)\n",
    "        # get global weights by averaging local weights\n",
    "        global_weights = average_weights(local_weights)\n",
    "        print(\"global wegiths: \", global_weights)\n",
    "\n",
    "        # update global weights\n",
    "        #global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)                # average losses from participated client\n",
    "        train_loss.append(loss_avg)                                     # save loss for this round\n",
    "        print(\"All results done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import multiprocessing\n",
    "\n",
    "# def client_train(args, train_dataset, logger,\n",
    "#                  test_dataset, idx, epoch, global_weights=None, result_queue=None):\n",
    "#     mask_time_prob = 0                                                          \n",
    "#     config = Data2VecAudioConfig.from_pretrained(args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "#     print(\"load from \", args.model_in_path)\n",
    "#     # with lock:\n",
    "#     #     model = Data2VecAudioForCTC.from_pretrained(args.model_in_path, config=config, args=args)\n",
    "#     model = Data2VecAudioForCTC.from_pretrained(args.model_in_path, config=config, args=args)\n",
    "#     print(\"model loaded\")                                                       \n",
    "#     # model.config.ctc_zero_infinity = True \n",
    "#     # global_model = copy.deepcopy(model.arbitrator)                              \n",
    "#     # if global_weights != None:                                                  \n",
    "#     #     global_model.load_state_dict(global_weights)                            \n",
    "#     # processor = Wav2Vec2Processor.from_pretrained(args.pretrain_name)\n",
    "#     # data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "#     return 0\n",
    "        \n",
    "# train_loss, test_wer = [], []\n",
    "# val_acc_list, net_list = [], []\n",
    "# cv_loss, cv_acc = [], []\n",
    "# print_every = 2\n",
    "# val_loss_pre, counter = 0, 0\n",
    "# global_weights = None                                                           # initial global_weights\n",
    "# # 創建進程間共享的 Queue 對象\n",
    "# # manager = mp.Manager()\n",
    "# # result_queue = manager.Queue()\n",
    "\n",
    "# train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 要读取的文件列表\n",
    "#     files = [\"file1.txt\", \"file2.txt\", \"file3.txt\"]\n",
    "\n",
    "#     # 创建进程池\n",
    "#     for epoch in range(2):\n",
    "        \n",
    "#         m = max(int(args.frac * args.num_users), 1)                                 # num of clients to train, min:1\n",
    "#         idxs_users = np.random.choice(range(args.num_users), m, replace=False)      # select by client_id\n",
    "#         pool = multiprocessing.Pool(processes=m)\n",
    "        \n",
    "#         # 使用进程池并行读取文件\n",
    "#         # results = pool.map(read_file, files)\n",
    "#         try:\n",
    "#             final_result = pool.starmap_async(\n",
    "#                 client_train, [(args, train_dataset, logger,\n",
    "#                                 test_dataset, idx, epoch, global_weights)\n",
    "#                             for idx in idxs_users])\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred while running local_model.update_weights(): {str(e)}\")\n",
    "#         finally:\n",
    "#             final_result.wait()\n",
    "#             results = final_result.get()\n",
    "#         # 输出结果\n",
    "#         for result in results:\n",
    "#             print(result)\n",
    "#     print(\"All results done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flower-speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
