{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 17:26:48.270880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/FedASR/dacs/federated/src/update.py:29: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here\n",
      "\n",
      "Experimental details:\n",
      "    Model     : data2vec\n",
      "    Global Rounds   : 2\n",
      "\n",
      "    Current Stage   : 1\n",
      "\n",
      "    Loss Type       : cel\n",
      "\n",
      "    Federated parameters:\n",
      "    Number of users    : 2\n",
      "    Fraction of users  : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-7cd3d56ce65492d2_*_of_00010.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-c5c36142e1357d21_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n",
      "Load data from local...\n",
      "| Start FL Training Stage 1|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 17:26:52.458644: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 17:26:52.683445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/FedASR/dacs/federated/src/update.py:29: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n",
      "/home/FedASR/dacs/federated/src/update.py:29: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating client training set for client  0 ...\n",
      "Generating client testing set for client  0 ...\n",
      "Generating client training set for client  1 ...\n",
      "Generating client testing set for client  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-61d2c01fd50e0c4f.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-3266f2669213b7f9.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-e8df470bb0b50834.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-985eb06188176881.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  tensor(0.5000)\n",
      "Current stage: 0\n",
      "lambda =  tensor(0.5000)\n",
      "Current stage: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `Data2VecAudioForCTC.forward` and have been ignored: array, path, text. If array, path, text are not expected by `Data2VecAudioForCTC.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Client  1  ready to train! |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `Data2VecAudioForCTC.forward` and have been ignored: text, path, array. If text, path, array are not expected by `Data2VecAudioForCTC.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Client  0  ready to train! |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 23\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 115\n",
      "  Number of trainable parameters = 309,101,600\n",
      "  0%|          | 0/115 [00:00<?, ?it/s]/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  \"`as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your \"\n",
      "  0%|          | 0/115 [00:00<?, ?it/s]\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 28\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 140\n",
      "  Number of trainable parameters = 309,101,600\n",
      "  0%|          | 0/140 [00:00<?, ?it/s]/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  \"`as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your \"\n",
      "  0%|          | 0/140 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "div() got an unexpected keyword argument 'rounding_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/multiprocessing/pool.py\", line 47, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/FedASR/dacs/federated/src/training.py\", line 27, in client_train\n    w, loss = local_model.update_weights(global_weights=global_weights, global_round=epoch)\n  File \"/home/FedASR/dacs/federated/src/update.py\", line 550, in update_weights\n    trainer.train()\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/trainer.py\", line 1666, in train\n    ignore_keys_for_eval=ignore_keys_for_eval,\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/trainer.py\", line 1929, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/trainer.py\", line 2699, in training_step\n    loss = self.compute_loss(model, inputs)\n  File \"/home/FedASR/dacs/federated/src/update.py\", line 57, in compute_loss\n    outputs = model(**inputs)\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/FedASR/dacs/federated/src/models.py\", line 400, in forward\n    return_dict=return_dict,\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 931, in forward\n    extract_features.shape[1], attention_mask, add_adapter=False\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 752, in _get_feature_vector_attention_mask\n    output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths, add_adapter=add_adapter)\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 736, in _get_feat_extract_output_lengths\n    input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n  File \"/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 733, in _conv_out_length\n    return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\nTypeError: div() got an unexpected keyword argument 'rounding_mode'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2151200/1958568108.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFL_STAGE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"| Start FL Training Stage 1|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mstage1_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m# Train ASR & AD Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"| FL Training Stage 1 Done|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2151200/1958568108.py\u001b[0m in \u001b[0;36mstage1_training\u001b[0;34m(args, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAGE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m                                                                  \u001b[0;31m# train ASR first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     global_weights = FL_training_rounds(args=args, model_in_path_root=args.model_out_path, model_out_path=args.model_out_path+\"_finetune\",\n\u001b[0;32m--> 100\u001b[0;31m                                         train_dataset=train_dataset, test_dataset=test_dataset)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# update global model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2151200/1958568108.py\u001b[0m in \u001b[0;36mFL_training_rounds\u001b[0;34m(args, model_in_path_root, model_out_path, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mfinal_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                     \u001b[0;31m# wait for all clients end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                            \u001b[0;31m# get results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                             \u001b[0;31m# for each participated clients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Flower-speechbrain/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: div() got an unexpected keyword argument 'rounding_mode'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from options import args_parser\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "import multiprocessing\n",
    "from update import update_network_weight, get_model_weight\n",
    "\n",
    "from training import client_train, centralized_training\n",
    "from update import ASRLocalUpdate\n",
    "\n",
    "def FL_training_rounds(args, model_in_path_root, model_out_path, train_dataset, test_dataset):\n",
    "    train_loss = []                                                                 # list for training loss\n",
    "    global_weights = None                                                           # initial global_weights\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(args.epochs)):                                          # train for given global rounds\n",
    "        print(f'\\n | Global Training Round : {epoch+1} |\\n')                        # print current round\n",
    "\n",
    "        m = max(int(args.frac * args.num_users), 1)                                 # num of clients to train, min:1\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)      # select by client_id\n",
    "        pool = multiprocessing.Pool(processes=m)\n",
    "\n",
    "        if args.STAGE == 0:                                                         # train ASR\n",
    "            local_weights_en = []                                                   # weight list for ASR encoder\n",
    "            local_weights_de = []                                                   # weight list for ASR decoder\n",
    "        else:                                                                       # train AD classifier or toggling network\n",
    "            local_weights = []                                                      # only 1 weight list needed\n",
    "        local_losses = []                                                           # losses of training clients of this round\n",
    "\n",
    "        try:\n",
    "            if (epoch == 0) and (args.STAGE == 2):                                  # start from global model to train toggling network\n",
    "                global_weights = get_model_weight(args=args, source_path=model_out_path + \"_global/final/\", network=\"toggling_network\")\n",
    "                                                                                    # local ASR and AD with global toggling network\n",
    "                                                                                    # get toggling_network weights from model in model_out_path + \"_global/final/\"\n",
    "            final_result = pool.starmap_async(client_train, [(args, model_in_path_root, model_out_path, train_dataset, test_dataset, idx,\n",
    "                                                                  epoch, global_weights) for idx in idxs_users])\n",
    "                                                                                    # train from model in model_in_path \n",
    "                                                                                    #                                 + \"_global/final/\", when stage=0\n",
    "                                                                                    #                                 + \"_client\" + str(idx) + \"_round\" + str(args.epochs-1) + \"/final/\", o.w.\n",
    "                                                                                    # or model in last round\n",
    "                                                                                    # final result in model_out_path + \"_client\" + str(client_id) + \"_round\" + str(global_round)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while using starmap_sync to run client_train: {str(e)}\")\n",
    "        \n",
    "        finally:\n",
    "            final_result.wait()                                                     # wait for all clients end\n",
    "            results = final_result.get()                                            # get results\n",
    "        \n",
    "        for idx in range(len(results)):                                             # for each participated clients\n",
    "            w, loss = results[idx]                                                  # function client_train returns w & loss\n",
    "            if args.STAGE == 0:                                                     # train ASR\n",
    "                local_weights_en.append(copy.deepcopy(w[0]))                        # save encoder weight for this client\n",
    "                local_weights_de.append(copy.deepcopy(w[1]))                        # save decoder weight for this client\n",
    "            else:                                                                   # train AD classifier or toggling network\n",
    "                local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        # aggregate weights\n",
    "        if args.STAGE == 0:                                                         # train ASR\n",
    "            global_weights = [average_weights(local_weights_en), average_weights(local_weights_de)]\n",
    "        else:                                                                       # train AD classifier or toggling network\n",
    "            global_weights = average_weights(local_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)                            # average losses from participated client\n",
    "        train_loss.append(loss_avg)                                                 # save loss for this round\n",
    "    return global_weights\n",
    "\n",
    "# FL stage 1: ASR & AD Classifier\n",
    "def stage1_training(args, train_dataset, test_dataset):\n",
    "    local_epoch = args.local_ep                                                     # save given number of local epoch\n",
    "    ##########################################################\n",
    "    # Centralized Training: train global ASR & AD Classifier #\n",
    "    ##########################################################\n",
    "    \"\"\"\n",
    "    args.local_ep = args.global_ep                                                  # use number of global epoch for global model\n",
    "    args.STAGE = 0                                                                  # train ASR first\n",
    "    centralized_training(args=args, model_in_path=args.pretrain_name, model_out_path=args.model_out_path+\"_finetune\", \n",
    "                         train_dataset=train_dataset, test_dataset=test_dataset, epoch=0)\n",
    "                                                                                    # train from pretrain, final result in args.model_out_path + \"_finetune\" + \"_global/final\"\n",
    "    args.STAGE = 1                                                                  # then train AD classifier\n",
    "    centralized_training(args=args, model_in_path=args.model_out_path+\"_finetune_global/final/\", \n",
    "                         model_out_path=args.model_out_path, train_dataset=train_dataset, test_dataset=test_dataset, epoch=0)\n",
    "                                                                                    # train from final result from last line, final result in args.model_out_path + \"_global/final\"\n",
    "    \"\"\"\n",
    "    ##########################################################\n",
    "    # FL: train local ASR & AD Classifier federally          #\n",
    "    ##########################################################\n",
    "    args.local_ep = local_epoch                                                     # use the given number of local epoch\n",
    "    args.STAGE = 0                                                                  # train ASR first\n",
    "    global_weights = FL_training_rounds(args=args, model_in_path_root=args.model_out_path, model_out_path=args.model_out_path+\"_finetune\",\n",
    "                                        train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "\n",
    "    # update global model\n",
    "    model = update_network_weight(args=args, source_path=args.model_out_path+\"_global/final/\", target_weight=global_weights, network=\"ASR\") \n",
    "                                                                                    # update ASR in source_path with given weights\n",
    "    model.save_pretrained(args.model_out_path+\"_FLASR_global/final\")\n",
    "    \n",
    "    args.STAGE = 1                                                                  # then train AD classifier\n",
    "    global_weights = FL_training_rounds(args=args, model_in_path_root=args.model_out_path+\"_finetune\", model_out_path=args.model_out_path,\n",
    "                                        train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "\n",
    "    # update global model\n",
    "    model = update_network_weight(args=args, source_path=args.model_out_path+\"_FLASR_global/final\", target_weight=global_weights, network=\"AD\")\n",
    "                                                                                    # update AD classifier in source_path with given weights\n",
    "    model.save_pretrained(args.model_out_path+\"_FLAD_global/final\")\n",
    "    \n",
    "    \n",
    "# FL stage 2: Toggling Network\n",
    "def stage2_training(args, train_dataset, test_dataset):\n",
    "    local_epoch = args.local_ep                                                     # save given number of local epoch\n",
    "    ##########################################################\n",
    "    # Centralized Training: train global Toggling Network    #\n",
    "    ##########################################################\n",
    "    \"\"\"\n",
    "    args.local_ep = args.global_ep                                                  # use number of global epoch for global model\n",
    "    centralized_training(args=args, model_in_path=args.model_in_path + \"_FLAD_global/final/\", model_out_path=args.model_out_path, \n",
    "                         train_dataset=train_dataset, test_dataset=test_dataset, epoch=0)\n",
    "                                                                                    # train from model_in_path + \"_FLAD_global/final/\" (aggregated ASR & AD)\n",
    "                                                                                    # final result in args.model_out_path + \"_global/final\"\n",
    "    \"\"\"\n",
    "    ##########################################################\n",
    "    # FL: train local Toggling Network federally             #\n",
    "    ##########################################################\n",
    "    args.local_ep = local_epoch                                                     # use the given number of local epoch\n",
    "    global_weights = FL_training_rounds(args=args, model_in_path_root=args.model_in_path, model_out_path=args.model_out_path,\n",
    "                                        train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "    # update global model\n",
    "    model = update_network_weight(args=args, source_path=args.model_out_path+\"_global/final\", target_weight=global_weights, network=\"toggling_network\")\n",
    "                                                                                    # update toggling_network in source_path with given weights\n",
    "    model.save_pretrained(args.model_out_path+\"_final_global/final\")\n",
    "\n",
    "def extract_emb(args, train_dataset, test_dataset):\n",
    "    if args.client_id == \"public\":\n",
    "        idx = \"public\"\n",
    "    else:\n",
    "        idx = int(args.client_id)\n",
    "    local_model = ASRLocalUpdate(args=args, dataset=train_dataset, global_test_dataset=test_dataset, \n",
    "                                 client_id=idx, model_in_path=args.model_in_path, model_out_path=None)\n",
    "                                                                                      # initial dataset of current client\n",
    "    local_model.extract_embs()\n",
    "                                                                                      # from model_in_path model, update certain part using given weight\n",
    "# if __name__ == '__main__':\n",
    "print(\"I'm here\")\n",
    "start_time = time.time()\n",
    "\n",
    "# define paths\n",
    "path_project = os.path.abspath('..')\n",
    "\n",
    "# args = args_parser()  \n",
    "import argparse                                                          # get configuration\n",
    "parser = argparse.ArgumentParser()\n",
    "# federated arguments (Notation for the arguments followed from paper)\n",
    "parser.add_argument('--epochs', type=int, default=2,\n",
    "                    help=\"number of rounds of training\")\n",
    "parser.add_argument('--num_users', type=int, default=2,\n",
    "                    help=\"number of users: K\")\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='the fraction of clients: C')\n",
    "parser.add_argument('--local_ep', type=int, default=5,\n",
    "                    help=\"the number of local epochs: E\")\n",
    "# model arguments\n",
    "parser.add_argument('--model', type=str, default='data2vec', help='model name')\n",
    "parser.add_argument('--dataset', type=str, default='adress', help=\"name \\\n",
    "                    of dataset\")\n",
    "parser.add_argument('--gpu', default=None, help=\"To use cuda, set \\\n",
    "                    to a specific GPU ID. Default set to use CPU.\")\n",
    "# additional arguments\n",
    "parser.add_argument('--pretrain_name', type=str, default='facebook/data2vec-audio-large-960h', help=\"str used to load pretrain model\")\n",
    "\n",
    "parser.add_argument('-lam', '--LAMBDA', type=float, default=0.5, help=\"Lambda for GRL\")\n",
    "parser.add_argument('-st', '--STAGE', type=int, default=1, help=\"Current training stage\")\n",
    "parser.add_argument('-fl_st', '--FL_STAGE', type=int, default=1, help=\"Current FL training stage\")\n",
    "parser.add_argument('-GRL', '--GRL', action='store_true', default=False, help=\"True: GRL\")\n",
    "parser.add_argument('-model_in', '--model_in_path', type=str, default=\"/home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall_client0_round1/\", help=\"Where the global model is saved\")\n",
    "parser.add_argument('-model_out', '--model_out_path', type=str, default=\"/home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall\", help=\"Where to save the model\")\n",
    "parser.add_argument('-log', '--log_path', type=str, default=\"wav2vec2-base-960h_linear_GRL.txt\", help=\"name for the txt file\")\n",
    "parser.add_argument('-csv', '--csv_path', type=str, default=\"wav2vec2-base-960h_GRL_0.5\", help=\"name for the csv file\")\n",
    "# 2023/01/08: loss type\n",
    "parser.add_argument('-ad_loss', '--AD_loss', type=str, default=\"cel\", help=\"loss to use for AD classifier\")\n",
    "# 2023/01/18: ckpt\n",
    "parser.add_argument('-ckpt', '--checkpoint', type=str, default=None, help=\"path to checkpoint\")\n",
    "# 2023/02/13: TOGGLE_RATIO\n",
    "parser.add_argument('-toggle_rt', '--TOGGLE_RATIO', type=float, default=0, help=\"To toggle more or less\")\n",
    "# 2023/02/15: GS_TAU, loss weight\n",
    "parser.add_argument('-gs_tau', '--GS_TAU', type=float, default=1, help=\"Tau for gumbel_softmax\")\n",
    "parser.add_argument('-w_loss', '--W_LOSS', type=float, default=None, nargs='+', help=\"weight for HC and AD\")\n",
    "# 2023/04/20\n",
    "parser.add_argument('-EXTRACT', '--EXTRACT', action='store_true', default=False, help=\"True: extract embs\")\n",
    "parser.add_argument('-client_id', '--client_id', type=str, default=\"public\", help=\"client_id: public, 0, or 1\")\n",
    "# 2023/04/24\n",
    "parser.add_argument('--global_ep', type=int, default=30, help=\"number for global model\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "exp_details(args)                                                               # print out details based on configuration\n",
    "\n",
    "train_dataset, test_dataset = get_dataset(args)                                 # get dataset\n",
    "# _, test_dataset = get_dataset(args)\n",
    "# train_dataset=test_dataset #å…ˆé€™æ¨£Debug\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "if args.EXTRACT != True:                                                        # Training\n",
    "    if args.FL_STAGE == 1:\n",
    "        print(\"| Start FL Training Stage 1|\")\n",
    "        stage1_training(args, train_dataset, test_dataset)                      # Train ASR & AD Classifier\n",
    "        print(\"| FL Training Stage 1 Done|\")\n",
    "\n",
    "    elif args.FL_STAGE == 2:\n",
    "        print(\"| Start FL Training Stage 2|\")\n",
    "        args.STAGE = 2\n",
    "        stage2_training(args, train_dataset, test_dataset)                      # Train Toggling Network\n",
    "        print(\"| FL Training Stage 2 Done|\")\n",
    "else:\n",
    "    extract_emb(args, train_dataset, test_dataset)\n",
    "\n",
    "print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flower-speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
