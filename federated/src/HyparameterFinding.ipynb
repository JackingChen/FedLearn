{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 11:07:41.206911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/FedASR/dacs/federated/src/update.py:32: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 更新2023/04/10\n",
    "# 1. csv2dataset函數裡面使用csv_path和root_path\n",
    "# 2. 在讀音檔的時候增加一個選項：scipy.io，讀起來會快很多但是不知道會不會影響到原來的效果\n",
    "\n",
    "# 大約10138MiB\n",
    "# =============================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import librosa\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "from transformers import Data2VecAudioConfig, HubertConfig, SEWDConfig, UniSpeechSatConfig\n",
    "from transformers import Data2VecAudioForCTC, HubertForCTC, SEWDForCTC, UniSpeechSatForCTC\n",
    "from jiwer import wer\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from typing import Dict\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import Data2VecAudioConfig\n",
    "from models import Data2VecAudioForCTC, Data2VecAudioForCTC_eval, DataCollatorCTCWithPadding\n",
    "from datasets import Dataset, load_from_disk\n",
    "import librosa\n",
    "from jiwer import wer\n",
    "import copy\n",
    "from transformers import Data2VecAudioConfig, Wav2Vec2Processor\n",
    "import copy\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from update import compute_metrics\n",
    "\n",
    "\n",
    "\n",
    "# set up trainer\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('-model', '--model_path', type=str, default=\"./saves/wav2vec2-base-960h_GRL_0.5\", help=\"Where the model is saved\")\n",
    "parser.add_argument('-opt', '--optimizer', type=str, default=\"adamw_hf\", help=\"The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor\")\n",
    "parser.add_argument('-MGN', '--max_grad_norm', type=float, default=1.0, help=\"Maximum gradient norm (for gradient clipping)\")\n",
    "parser.add_argument('-model_type', '--model_type', type=str, default=\"data2vec\", help=\"Type of the model\")\n",
    "parser.add_argument('-sr', '--sampl_rate', type=float, default=16000, help=\"librosa read smping rate\")\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=1e-5, help=\"Learning rate\")\n",
    "parser.add_argument('-RD', '--root_dir', default='/mnt/Internal/FedASR/Data/ADReSS-IS2020-data', help=\"Learning rate\")\n",
    "parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help=\"number of rounds of training\")\n",
    "parser.add_argument('--num_users', type=int, default=100,\n",
    "                    help=\"number of users: K\")\n",
    "parser.add_argument('--frac', type=float, default=0.1,\n",
    "                    help='the fraction of clients: C')\n",
    "parser.add_argument('--local_ep', type=int, default=10,\n",
    "                    help=\"the number of local epochs: E\")\n",
    "# model arguments\n",
    "parser.add_argument('--model', type=str, default='data2vec', help='model name')\n",
    "# other arguments\n",
    "parser.add_argument('--dataset', type=str, default='adress', help=\"name \\\n",
    "                    of dataset\")\n",
    "parser.add_argument('--gpu', default=None, help=\"To use cuda, set \\\n",
    "                    to a specific GPU ID. Default set to use CPU.\")\n",
    "# additional arguments\n",
    "parser.add_argument('--pretrain_name', type=str, default='facebook/data2vec-audio-large-960h', help=\"str used to load pretrain model\")\n",
    "\n",
    "parser.add_argument('-lam', '--LAMBDA', type=float, default=0.5, help=\"Lambda for GRL\")\n",
    "parser.add_argument('-st', '--STAGE', type=int, default=0, help=\"Current training stage\")\n",
    "parser.add_argument('-fl_st', '--FL_STAGE', type=int, default=1, help=\"Current FL training stage\")\n",
    "parser.add_argument('-GRL', '--GRL', action='store_true', default=False, help=\"True: GRL\")\n",
    "parser.add_argument('-model_in', '--model_in_path', type=str, default=\"./saves/wav2vec2-base-960h_GRL_0.5/checkpoint-14010/\", help=\"Where the global model is saved\")\n",
    "parser.add_argument('-model_out', '--model_out_path', type=str, default=\"./saves/wav2vec2-base-960h_linear_GRL\", help=\"Where to save the model\")\n",
    "parser.add_argument('-log', '--log_path', type=str, default=\"wav2vec2-base-960h_linear_GRL.txt\", help=\"name for the txt file\")\n",
    "parser.add_argument('-csv', '--csv_path', type=str, default=\"wav2vec2-base-960h_GRL_0.5\", help=\"name for the csv file\")\n",
    "# 2023/01/08: loss type\n",
    "parser.add_argument('-ad_loss', '--AD_loss', type=str, default=None, help=\"loss to use for AD classifier\")\n",
    "# 2023/01/18: ckpt\n",
    "parser.add_argument('-ckpt', '--checkpoint', type=str, default=None, help=\"path to checkpoint\")\n",
    "# 2023/02/13: TOGGLE_RATIO\n",
    "parser.add_argument('-toggle_rt', '--TOGGLE_RATIO', type=float, default=0, help=\"To toggle more or less\")\n",
    "# 2023/02/15: GS_TAU, loss weight\n",
    "parser.add_argument('-gs_tau', '--GS_TAU', type=float, default=1, help=\"Tau for gumbel_softmax\")\n",
    "parser.add_argument('-w_loss', '--W_LOSS', type=float, default=None, nargs='+', help=\"weight for HC and AD\")\n",
    "# 2023/04/20\n",
    "parser.add_argument('-EXTRACT', '--EXTRACT', action='store_true', default=False, help=\"True: extract embs\")\n",
    "parser.add_argument('-client_id', '--client_id', type=str, default=\"public\", help=\"client_id: public, 0, or 1\")\n",
    "# 2023/04/24\n",
    "parser.add_argument('--global_ep', type=int, default=30, help=\"number for global model\")\n",
    "    \n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "os.environ['DACS_codeRoot'] = '/home/FedASR/dacs/'\n",
    "os.environ['DACS_dataRoot'] = '/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2,3\"\n",
    "from utils import csv2dataset, get_dataset, average_weights, exp_details\n",
    "\n",
    "#model_out_dir = args.model_path # where to save model\n",
    "model_type = args.model_type                # what type of the model\n",
    "lr = args.learning_rate                     # learning rate\n",
    "optim = args.optimizer                      # opt\n",
    "max_grad_norm = args.max_grad_norm          # max_grad_norm\n",
    "\n",
    "\n",
    "def map_to_result(batch, model):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = model.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = model.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "class CustomTrainer(Trainer):    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            \"\"\"\n",
    "            How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "            Subclass and override for custom behavior.\n",
    "            \"\"\"\n",
    "            #dementia_labels = inputs.pop(\"dementia_labels\") # pop 出來就會不見?\n",
    "            \n",
    "            if self.label_smoother is not None and \"labels\" in inputs:\n",
    "                labels = inputs.pop(\"labels\")\n",
    "            else:\n",
    "                labels = None\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            # Save past state if it exists\n",
    "            # TODO: this needs to be fixed and made cleaner later.\n",
    "            if self.args.past_index >= 0:\n",
    "                self._past = outputs[self.args.past_index]\n",
    "\n",
    "            if labels is not None:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "            else:\n",
    "                # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "                loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.state.log_history.append(output)\n",
    "        \n",
    "        LOG_DIR=\"/home/FedASR/dacs/federated/logs\"\n",
    "        # write to txt file\n",
    "        file_object = open(LOG_DIR + args.log_path, 'a')\n",
    "        # Append at the end of file\n",
    "        file_object.write(json.dumps(output) + '\\n')\n",
    "        # Close the file\n",
    "        file_object.close()\n",
    "\n",
    "        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
    "\n",
    "from datasets import load_metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  tensor(0.5000)\n",
      "Current stage: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-large-960h were not used when initializing Data2VecAudioForCTC: ['data2vec_audio.masked_spec_embed']\n",
      "- This IS expected if you are initializing Data2VecAudioForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecAudioForCTC were not initialized from the model checkpoint at facebook/data2vec-audio-large-960h and are newly initialized: ['arbitrator.bias', 'dementia_head.weight', 'arbitrator.weight', 'criterion_similar.fc.weight', 'dementia_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-b6f4c0d2143105d5_*_of_00010.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-f07dc6d726972452_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n",
      "Load data from local...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f3465ee86941eba7991b5d1e3a0da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3936135/1756901696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_to_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WER of \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_str\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m         }\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3002\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3004\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3005\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3006\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3356\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3357\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3358\u001b[0;31m                         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_function_on_filtered_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3359\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3260\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m                 processed_inputs = {\n",
      "\u001b[0;32m/tmp/ipykernel_3936135/1753230601.py\u001b[0m in \u001b[0;36mmap_to_result\u001b[0;34m(batch, model)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mpred_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# args.model_in_path=\"/home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall_FLASR_global/final/\"\n",
    "args.model_in_path='facebook/data2vec-audio-large-960h'\n",
    "mask_time_prob = 0                                                              # change config to avoid training stopping\n",
    "config = Data2VecAudioConfig.from_pretrained(args.model_in_path, mask_time_prob=mask_time_prob)\n",
    "                                                                                # use pre-trained config\n",
    "trained_model = Data2VecAudioForCTC.from_pretrained(args.model_in_path, config=config, args=args)\n",
    "                                                                                # load from source\n",
    "trained_model.config.ctc_zero_infinity = True                                           # to avoid inf values\n",
    "\n",
    "train_dataset, test_dataset = get_dataset(args) \n",
    "\n",
    "result = test_dataset.map(map_to_result,trained_model)\n",
    "print(\"WER of \", args.pretrain_name, \" : \", wer(result[\"text\"], result[\"pred_str\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch, model):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = model.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = model.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 395, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in test_dataset:\n",
    "    # batch=test_dataset[0]\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = trained_model(input_values).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Model     : data2vec\n",
      "    Global Rounds   : 10\n",
      "\n",
      "    Current Stage   : 0\n",
      "\n",
      "    Loss Type       : None\n",
      "\n",
      "    Federated parameters:\n",
      "    Number of users    : 100\n",
      "    Fraction of users  : 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/preprocessor_config.json from cache at /home/FedASR/.cache/huggingface/transformers/a8b4bf67229a286620328df7e382c6b531d7ece801f213cb7d5e4033cc83f99b.bef560b27c62cea1af8278853fdffeaf0141c9c44f4298df07ba06cdf6f8f963\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"Wav2Vec2Processor\",\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "facebook/data2vec-audio-large-960h does not contain a file named https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/added_tokens.json.\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/vocab.json from cache at /home/FedASR/.cache/huggingface/transformers/372a98c2d55cb994819401be01f9c702fc9bd84043228e7ea16781afe07679a4.7c838a0a103758bad6ef4922531682da23a8b1c45d25f8d8e7a6d857c0b26544\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/tokenizer_config.json from cache at /home/FedASR/.cache/huggingface/transformers/c54f72dd2ec0fcf06a4cf658198e62509f88e6ae7662bc985ddce52fa7221836.1c95dfde9a2c9be07efd0f32dc4d19348c74abfd09851c4c073f20bc649af920\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/special_tokens_map.json from cache at /home/FedASR/.cache/huggingface/transformers/4e0025830be5b3bb58c4e6544c2dc3f0b21a7aa3fba77ca0586410625e0a9691.9d6cd81ef646692fb1c169a880161ea1cb95f49694f220aced9b704b457e51dd\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-b6f4c0d2143105d5_*_of_00010.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-f07dc6d726972452_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n",
      "Load data from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/config.json from cache at /home/FedASR/.cache/huggingface/transformers/a5e291023d6dd7ec0034390cee6d97f07e340fb24c68c7b5f3ec8d017a6fd29d.ed9b9e83fb80348aa91a073138fc7a0f44e669fc412c9c4bc98857f45bfd4330\n",
      "Model config Data2VecAudioConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Data2VecAudioForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_pos_kernel_size\": 19,\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0,\n",
      "  \"model_type\": \"data2vec-audio\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 5,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file /home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall_FLASR_global/final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  tensor(0.5000)\n",
      "Current stage: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Data2VecAudioForCTC.\n",
      "\n",
      "All the weights of Data2VecAudioForCTC were initialized from the model checkpoint at /home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall_FLASR_global/final/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Data2VecAudioForCTC for predictions without further training.\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-6300789777e9149b.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-30b6c539e31bb0a4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating client training set for client  0 ...\n",
      "Generating client testing set for client  0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/preprocessor_config.json from cache at /home/FedASR/.cache/huggingface/transformers/a8b4bf67229a286620328df7e382c6b531d7ece801f213cb7d5e4033cc83f99b.bef560b27c62cea1af8278853fdffeaf0141c9c44f4298df07ba06cdf6f8f963\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"Wav2Vec2Processor\",\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "facebook/data2vec-audio-large-960h does not contain a file named https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/added_tokens.json.\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/vocab.json from cache at /home/FedASR/.cache/huggingface/transformers/372a98c2d55cb994819401be01f9c702fc9bd84043228e7ea16781afe07679a4.7c838a0a103758bad6ef4922531682da23a8b1c45d25f8d8e7a6d857c0b26544\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/tokenizer_config.json from cache at /home/FedASR/.cache/huggingface/transformers/c54f72dd2ec0fcf06a4cf658198e62509f88e6ae7662bc985ddce52fa7221836.1c95dfde9a2c9be07efd0f32dc4d19348c74abfd09851c4c073f20bc649af920\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/special_tokens_map.json from cache at /home/FedASR/.cache/huggingface/transformers/4e0025830be5b3bb58c4e6544c2dc3f0b21a7aa3fba77ca0586410625e0a9691.9d6cd81ef646692fb1c169a880161ea1cb95f49694f220aced9b704b457e51dd\n",
      "loading configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/config.json from cache at /home/FedASR/.cache/huggingface/transformers/a5e291023d6dd7ec0034390cee6d97f07e340fb24c68c7b5f3ec8d017a6fd29d.ed9b9e83fb80348aa91a073138fc7a0f44e669fc412c9c4bc98857f45bfd4330\n",
      "Model config Data2VecAudioConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Data2VecAudioForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_pos_kernel_size\": 19,\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0,\n",
      "  \"model_type\": \"data2vec-audio\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 5,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/pytorch_model.bin from cache at /home/FedASR/.cache/huggingface/transformers/893180bb50d72ee90ecf687b3843c97d8986acff4869e620d47a4f5df9e7883c.7974d1dd90bde11cb69debf8299e7075403717e3950d6b3fb90ee34940a0ba48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  tensor(0.5000)\n",
      "Current stage: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-large-960h were not used when initializing Data2VecAudioForCTC: ['data2vec_audio.masked_spec_embed']\n",
      "- This IS expected if you are initializing Data2VecAudioForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecAudioForCTC were not initialized from the model checkpoint at facebook/data2vec-audio-large-960h and are newly initialized: ['dementia_head.bias', 'criterion_similar.fc.weight', 'dementia_head.weight', 'arbitrator.weight', 'arbitrator.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Data2VecAudioForCTC.forward` and have been ignored: array, path, text. If array, path, text are not expected by `Data2VecAudioForCTC.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Client  0  ready to train! |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 543\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1810\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  84/1810 14:29 < 5:04:54, 0.09 it/s, Epoch 0.46/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from update import update_network_weight\n",
    "def get_model_weight(args, source_path, network):                                   # get \"network\" weights from model in source_path\n",
    "    mask_time_prob = 0                                                              # change config to avoid training stopping\n",
    "    config = Data2VecAudioConfig.from_pretrained(args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "                                                                                    # use pre-trained config\n",
    "    model = Data2VecAudioForCTC.from_pretrained(source_path, config=config, args=args)\n",
    "                                                                                    # load from source\n",
    "    model.config.ctc_zero_infinity = True                                           # to avoid inf values\n",
    "\n",
    "    if network == \"ASR\":                                                            # get ASR weights\n",
    "        return_weights = [copy.deepcopy(model.data2vec_audio.state_dict()), copy.deepcopy(model.lm_head.state_dict())]\n",
    "    elif network == \"AD\":                                                           # get AD classifier weights\n",
    "        return_weights = copy.deepcopy(model.dementia_head.state_dict())\n",
    "    elif network == \"toggling_network\":                                             # get toggling network weights\n",
    "        return_weights = copy.deepcopy(model.arbitrator.state_dict())  \n",
    "    \n",
    "    return return_weights\n",
    "\n",
    "class ASRLocalUpdate(object):\n",
    "    def __init__(self, args, dataset, global_test_dataset, client_id, \n",
    "                 model_in_path, model_out_path):\n",
    "        self.args = args                                                            # given configuration\n",
    "        self.client_train_dataset = self.train_split(dataset, client_id)            # get subset of training set (dataset of THIS client)\n",
    "        \n",
    "        self.device = 'cuda' if args.gpu else 'cpu'                                 # use gpu or cpu\n",
    "        \n",
    "        self.global_test_dataset = global_test_dataset\n",
    "        self.client_test_dataset = self.test_split(global_test_dataset, client_id)  # get subset of testing set (dataset of THIS client)\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(args.pretrain_name)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.client_id = client_id\n",
    "\n",
    "        self.model_in_path = model_in_path                                          # no info for client_id & global_round\n",
    "        self.model_out_path = model_out_path                                        # no info for client_id & global_round\n",
    "\n",
    "    def train_split(self, dataset, client_id):\n",
    "        # generate sub- training set for given user-ID\n",
    "        if client_id == \"public\":                                                   # get spk_id for public dataset, 54 PAR (50% of all training set)\n",
    "            client_spks = ['S086', 'S021', 'S018', 'S156', 'S016', 'S077', 'S027', 'S116', 'S143', 'S082', 'S039', 'S150', 'S004', 'S126', 'S137', \n",
    "            'S097', 'S128', 'S059', 'S096', 'S081', 'S135', 'S094', 'S070', 'S049', 'S080', 'S040', 'S076', 'S093', 'S141', 'S034', 'S056', 'S090', \n",
    "            'S130', 'S092', 'S055', 'S019', 'S154', 'S017', 'S114', 'S100', 'S036', 'S029', 'S127', 'S073', 'S089', 'S051', 'S005', 'S151', 'S003', \n",
    "            'S033', 'S007', 'S084', 'S043', 'S009']                                 # 27 AD + 27 HC\n",
    "\n",
    "        elif client_id == 0:                                                        # get spk_id for client 1, 27 PAR (25% of all training set)\n",
    "            client_spks = ['S058', 'S030', 'S064', 'S104', 'S048', 'S118', 'S122', 'S001', 'S087', 'S013', 'S025', 'S083', 'S067', 'S068', 'S111', \n",
    "            'S028', 'S015', 'S108', 'S095', 'S002', 'S072', 'S020', 'S148', 'S144', 'S110', 'S124', 'S129']\n",
    "                                                                                    # 13 AD + 14 HC\n",
    "        elif client_id == 1:                                                        # get spk_id for client 2, 27 PAR (25% of all training set)  \n",
    "            client_spks = ['S071', 'S136', 'S140', 'S145', 'S032', 'S101', 'S103', 'S139', 'S038', 'S153', 'S035', 'S011', 'S132', 'S006', 'S149', \n",
    "            'S041', 'S079', 'S107', 'S063', 'S061', 'S125', 'S062', 'S012', 'S138', 'S024', 'S052', 'S142']\n",
    "                                                                                    # 14 AD + 13 HC\n",
    "        else:\n",
    "            print(\"Train with whole dataset!!\")\n",
    "            return dataset\n",
    "        \n",
    "        print(\"Generating client training set for client \", str(client_id), \"...\")\n",
    "        client_train_dataset = dataset.filter(lambda example: example[\"path\"].startswith(tuple(client_spks)))\n",
    "        \n",
    "        return client_train_dataset\n",
    "    \n",
    "    def test_split(self, dataset, client_id):\n",
    "        # generate sub- testing set for given user-ID\n",
    "        if client_id == \"public\":                                                   # get spk_id for public dataset, 24 PAR (50% of all testing set)\n",
    "            client_spks = ['S197', 'S163', 'S193', 'S169', 'S196', 'S184', 'S168', 'S205', 'S185', 'S171', 'S204', 'S173', 'S190', 'S191', 'S203', \n",
    "                           'S180', 'S165', 'S199', 'S160', 'S175', 'S200', 'S166', 'S177', 'S167']                                # 12 AD + 12 HC\n",
    "\n",
    "        elif client_id == 0:                                                        # get spk_id for client 1, 12 PAR (25% of all testing set)\n",
    "            client_spks = ['S198', 'S182', 'S194', 'S161', 'S195', 'S170', 'S187', 'S192', 'S178', 'S201', 'S181', 'S174']\n",
    "                                                                                    # 6 AD + 6 HC\n",
    "        elif client_id == 1:                                                        # get spk_id for client 2, 12 PAR (25% of all testing set)  \n",
    "            client_spks = ['S179', 'S188', 'S202', 'S162', 'S172', 'S183', 'S186', 'S207', 'S189', 'S164', 'S176', 'S206']\n",
    "                                                                                    # 6 AD + 6 HC\n",
    "        else:\n",
    "            print(\"Test with whole dataset!!\")\n",
    "            return dataset\n",
    "        \n",
    "        print(\"Generating client testing set for client \", str(client_id), \"...\")\n",
    "        client_test_dataset = dataset.filter(lambda example: example[\"path\"].startswith(tuple(client_spks)))\n",
    "        \n",
    "        return client_test_dataset\n",
    "    \n",
    "    def record_result(self, trainer, result_folder):                                # save training loss, testing loss, and testing wer\n",
    "        logger = SummaryWriter('../logs/' + result_folder.split(\"/\")[-1])           # use name of this model as folder's name\n",
    "\n",
    "        for idx in range(len(trainer.state.log_history)):\n",
    "            if \"loss\" in trainer.state.log_history[idx].keys():                     # add in training loss, epoch*100 to obtain int\n",
    "                logger.add_scalar('Loss/train', trainer.state.log_history[idx][\"loss\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "\n",
    "            elif \"eval_loss\" in trainer.state.log_history[idx].keys():              # add in testing loss & WER, epoch*100 to obtain int\n",
    "                logger.add_scalar('Loss/test', trainer.state.log_history[idx][\"eval_loss\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "                logger.add_scalar('wer/test', trainer.state.log_history[idx][\"eval_wer\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "\n",
    "            else:                                                                   # add in final training loss, epoch*100 to obtain int\n",
    "                logger.add_scalar('Loss/train', trainer.state.log_history[idx][\"train_loss\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "        logger.close()\n",
    "\n",
    "    def update_weights(self, global_weights, global_round):\n",
    "        if global_weights == None:                                                  # train from model from model_in_path\n",
    "            mask_time_prob = 0                                                      # change config to avoid training stopping\n",
    "            config = Data2VecAudioConfig.from_pretrained(self.args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "                                                                                    # use pre-trained config\n",
    "            model = Data2VecAudioForCTC.from_pretrained(self.model_in_path, config=config, args=self.args)\n",
    "            model.config.ctc_zero_infinity = True                                   # to avoid inf values\n",
    "        else:                                                                       # update train model using given weight\n",
    "            if self.args.STAGE == 0:                                                # train ASR\n",
    "                model = update_network_weight(args=self.args, source_path=self.model_in_path, target_weight=global_weights, network=\"ASR\")                    \n",
    "                                                                                    # from model from model_in_path, update ASR's weight\n",
    "            elif self.args.STAGE == 1:                                              # train AD classifier\n",
    "                model = update_network_weight(args=self.args, source_path=self.model_in_path, target_weight=global_weights, network=\"AD\")           \n",
    "                                                                                    # from model from model_in_path, update AD classifier's weight\n",
    "            elif self.args.STAGE == 2:                                              # train toggling network\n",
    "                model = update_network_weight(args=self.args, source_path=self.model_in_path, target_weight=global_weights, network=\"toggling_network\")             \n",
    "                                                                                    # from model from model_in_path, update arbitrator's weight\n",
    "        global log_path\n",
    "        log_path = self.args.log_path\n",
    "\n",
    "        model.train()\n",
    "        if self.args.STAGE == 0:                                                    # fine-tune ASR\n",
    "            lr = 1e-5\n",
    "        elif self.args.STAGE == 1:                                                  # train AD classifier\n",
    "            lr = 1e-4\n",
    "        elif self.args.STAGE == 2:                                                  # train toggling network\n",
    "            lr = 1e-3\n",
    "\n",
    "        if self.client_id == \"public\":                                              # model train with public dataset, name end with \"_global\"\n",
    "            save_path = self.model_out_path + \"_global\"\n",
    "        else:\n",
    "            save_path = self.model_out_path + \"_client\" + str(self.client_id) + \"_round\" + str(global_round)\n",
    "                                                                                    # for local models, record info for id & num_round\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=save_path,\n",
    "            group_by_length=True,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            num_train_epochs=self.args.local_ep, #self.args.local_ep\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True, \n",
    "            save_steps=500, # 500\n",
    "            eval_steps=500, # 500\n",
    "            logging_steps=10, # 500\n",
    "            learning_rate=lr, # self.args.lr\n",
    "            weight_decay=0.005,\n",
    "            warmup_steps=1000,\n",
    "            save_total_limit=2,\n",
    "            log_level='debug',\n",
    "            logging_strategy=\"steps\",\n",
    "            #adafactor=True,            # default:false. Whether or not to use transformers.Adafactor optimizer instead of transformers.AdamW\n",
    "            #fp16_full_eval=True,      # to save memory\n",
    "            #max_grad_norm=0.5\n",
    "        )\n",
    "        global processor\n",
    "        processor = self.processor\n",
    "        # self.debug_mdl=model\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            data_collator=self.data_collator,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=self.client_train_dataset,\n",
    "            eval_dataset=self.global_test_dataset,\n",
    "            tokenizer=self.processor.feature_extractor,\n",
    "        )\n",
    "\n",
    "        print(\" | Client \", str(self.client_id), \" ready to train! |\")\n",
    "        trainer.train()\n",
    "        return trainer.model\n",
    "\n",
    "        trainer.save_model(save_path + \"/final\")                                    # save final model\n",
    "        self.record_result(trainer, save_path)                           # save training loss, testing loss, and testing wer\n",
    "\n",
    "        # get \"network\" weights from model in source_path\n",
    "        if self.args.STAGE == 0:                                                    # train ASR\n",
    "            return_weights = get_model_weight(args=self.args, source_path=save_path + \"/final/\", network=\"ASR\")\n",
    "        elif self.args.STAGE == 1:                                                  # train AD classifier\n",
    "            return_weights = get_model_weight(args=self.args, source_path=save_path + \"/final/\", network=\"AD\")\n",
    "        elif self.args.STAGE == 2:                                                  # train toggling_network\n",
    "            return_weights = get_model_weight(args=self.args, source_path=save_path + \"/final/\", network=\"toggling_network\")  \n",
    "         \n",
    "        return return_weights, trainer.state.log_history[-1][\"train_loss\"]          # return weight, average losses for this round\n",
    "\n",
    "    def extract_embs(self):                                                         # extract emb. using model in args.model_in_path\n",
    "        # load model\n",
    "        mask_time_prob = 0                                                          # change config to avoid code from stopping\n",
    "        config = Data2VecAudioConfig.from_pretrained(self.args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "        model = Data2VecAudioForCTC_eval.from_pretrained(self.args.model_in_path, config=config, args=self.args)\n",
    "        processor = self.processor\n",
    "\n",
    "        # get emb.s, masks... 1 sample by 1 sample for client test\n",
    "        df = map_to_result(self.client_test_dataset[0], processor, model, 0)\n",
    "        for i in range(len(self.client_test_dataset) - 1):\n",
    "            df2 = map_to_result(self.client_test_dataset[i+1], processor, model, i+1)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "            print(\"\\r\"+ str(i), end=\"\")\n",
    "\n",
    "        csv_path = \"./results/\" + self.args.csv_path + \".csv\"\n",
    "        df.to_csv(csv_path)\n",
    "        print(\"Testing data Done\")\n",
    "\n",
    "        # get emb.s, masks... 1 sample by 1 sample for client train\n",
    "        df = map_to_result(self.client_train_dataset[0], processor, model, 0)\n",
    "        for i in range(len(self.client_train_dataset) - 1):\n",
    "            df2 = map_to_result(self.client_train_dataset[i+1], processor, model, i+1)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "            print(\"\\r\"+ str(i), end=\"\")\n",
    "\n",
    "        csv_path = \"./results/\" + self.args.csv_path + \"_train.csv\"\n",
    "        df.to_csv(csv_path)\n",
    "        print(\"Training data Done\")\n",
    "\n",
    "        print(self.args.csv_path + \" All Done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exp_details(args)                                                               # print out details based on configuration\n",
    "# 吃資料\n",
    "train_dataset, test_dataset = get_dataset(args)                                 # get dataset\n",
    "\n",
    "\n",
    "\n",
    "# train_data = csv2dataset(PATH = '{}/clips/'.format(args.root_dir),\n",
    "#                          path = \"{}/mid_csv/train.csv\".format(args.root_dir)) #!!! librosa在load的時候非常慢，大約7分47秒讀完1869個file\n",
    "# dev_data = csv2dataset(PATH = '{}/clips/'.format(args.root_dir),\n",
    "#                        path = \"{}/mid_csv/dev.csv\".format(args.root_dir))\n",
    "# test_data = csv2dataset(PATH = '{}/clips/'.format(args.root_dir),\n",
    "#                         path = \"{}/mid_csv/test.csv\".format(args.root_dir))\n",
    "# 吃global\n",
    "client_id=0\n",
    "model_in_path=\"facebook/data2vec-audio-large-960h\"\n",
    "global_mdl_path=\"/home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall_FLASR\"\n",
    "model_out_path=\"/home/FedASR/dacs/federated/save/\"\n",
    "global_weights = get_model_weight(args=args, source_path=global_mdl_path + \"_global/final/\", network=\"ASR\")\n",
    "                                                                    # local ASR and AD with global toggling network\n",
    "                                                                    # get toggling_network weights from model in model_out_path + \"_global/final/\"\n",
    "# 選資料\n",
    "local_model = ASRLocalUpdate(args=args, dataset=train_dataset, global_test_dataset=test_dataset, \n",
    "                                 client_id=client_id, model_in_path=model_in_path, model_out_path=model_out_path)\n",
    "                                                                                      # initial dataset of current client\n",
    "# 訓練模型\n",
    "trained_model = local_model.update_weights(global_weights=global_weights, global_round=0) \n",
    "# evaluate\n",
    "result = test_dataset.map(map_to_result)\n",
    "print(\"WER of \", args.pretrain_name, \" : \", wer(result[\"text\"], result[\"pred_str\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.STAGE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flower-speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
